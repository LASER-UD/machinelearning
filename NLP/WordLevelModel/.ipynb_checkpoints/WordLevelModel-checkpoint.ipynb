{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Texto alternativo](https://laserud.co/wp-content/uploads/2020/05/cropped-LOGOLASER-1.jpg \"Grupo LASER\")\n",
    "# Modelo Lingüistico a Nivel de Palabras y Generación de Texto\n",
    "Un modelo de lenguaje puede predecir la probabilidad de la siguiente palabra en una secuencia, basado en las palabras ya observadas de la secuencia.\n",
    "\n",
    "Un modelo de lenguaje estadístico asigna una probabilidad a una secuencia de $ m $ palabras $ P(w_1, w_2, … , w_m) $ mediante una distribución de probabilidad. Para muchas aplicaciones de NLP es importante tener una forma de estimar la verosimilitud de una frase dentro de una oración. Dentro de las aplicaciones se puede destacar el reconocimiento de voz, traducción automática, etiquetado del discurso, reconocimiento de escritura, entre otras.\n",
    "\n",
    "La escasez de datos es un problema importante para la construcción de modelos de lenguaje estadísticos. La mayoría de las posibles secuencias de palabras no serán observadas durante el proceso de entrenamiento. Por lo cual se hace la hipótesis de que la probabilidad de una palabra solo depende de las $ n $ palabras anteriores. Esto se conoce como un modelo de N-grama.\n",
    "\n",
    "Los modelos de redes neuronales son los métodos preferidos para desarrollar modelos estadísticos de lenguaje porque pueden utilizar una representación distribuida en la que diferentes palabras con resultados similares tienen una representación similar y porque pueden utilizar un amplio contexto de palabras recientemente observadas al hacer predicciones.\n",
    "\n",
    "Empezaremos cargando los modulos necesarios para llevar a cabo el ejemplo de hoy. Junto con la función definida, a lo largo del tutorial se utilizarán y se explicará su funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "from pickle import dump, load\n",
    "\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = np.argmax(model.predict(encoded), axis=-1)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, para este ejercicio elegimos un texto simple, Platero y yo, escrito por el poeta Español Juan Ramón Jiménez en 1914. Hemos decidido utilizar este texto para el ejemplo de hoy, pues es una obra simple, corta y el texto crudo puede ser descargado desde el proyecto Gutenberg en: https://www.gutenberg.org/ebooks/39209.\n",
    "\n",
    "Al descargar el archivo de texto, podemos darnos cuenta que el archivo elegido contiene encabezado y pie de página, y no estamos interesados en esta información. Así que abrimos el archivo y borramos esta pequeña sección del archivo.\n",
    "\n",
    "El primer paso necesario es cargar el texto en memoria. Como mencionamos anteriormente, se eligió un texto corto, de tal manera que no tendremos dificultades para el manejo del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://github.com/LASER-UD/machinelearning/blob/main/NLP/WordLevelModel/platero.txt?raw=true\"\n",
    "text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpiar el Texto\n",
    "Necesitamos transformar el texto crudo en una secuencia de tokens o palabras que podamos usar como fuente para entrenar el modelo. Basados en la revisión del texto anteriormente mencionada vamos a ejecutar algunas operaciones para dejar el texto en el formato deseado.\n",
    "-\tDividir el texto en palabras basados en los espacios en blanco.\n",
    "-\tEliminar toda la puntuación del texto.\n",
    "-\tEliminar todas las palabras que no sean alfabéticas para asegurarnos que no queden signos de puntuación aislados.\n",
    "-\tNormalizar todas las palabras a minúsculas para reducir el tamaño del vocabulario.\n",
    "\n",
    "El tamaño del vocabulario es un factor importante en el modelado del lenguaje. Un vocabulario más pequeño da lugar a un modelo más pequeño que se entrena más rápido.\n",
    "\n",
    "Vamos a ejecutar las operaciones de procesamiento mencionadas anteriormente. Primero dividimos manualmente el texto en palabras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffADVERTENCIA', 'Á', 'LOS', 'HOMBRES', 'QUE', 'LEAN', 'ESTE', 'LIBRO', 'PARA', 'NIÑOS', 'Este', 'breve', 'libro,', 'en', 'donde', 'la', 'alegría', 'y', 'la', 'pena']\n"
     ]
    }
   ],
   "source": [
    "filters = string.punctuation+ '¿¡'\n",
    "tokens = doc.split()\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la puntuación del texto, agregando al filtro los signos '¿¡', que como ya hemos visto no son utilizados en el idioma inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', filters)\n",
    "tokens = [w.translate(table) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos el texto a minúsculas y verificamos que todos los tokens sean alfabéticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['á', 'los', 'hombres', 'que', 'lean', 'este', 'libro', 'para', 'niños', 'este', 'breve', 'libro', 'en', 'donde', 'la', 'alegría', 'y', 'la', 'pena', 'son']\n",
      "Total Tokens: 12120\n",
      "Unique Tokens: 3277\n"
     ]
    }
   ],
   "source": [
    "tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "print(tokens[:20])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que tenemos un total cercano a 12100 palabras en el texto y un vocabulario de  aproximadamente 3200 palabras. Este es un vocabulario manejable para ajustar nuestro modelo.\n",
    "\n",
    "### Organizar el Texto\n",
    "\n",
    "Ahora debemos organizar los tokens disponibles de acuerdo con el tamaño de entrada elegido. En este caso se tomarán secuencias de ¿50? palabras y una salida. Para organizar los tokens, separaremos cada uno de ellos mediante espacios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 12069\n"
     ]
    }
   ],
   "source": [
    "IN_WORDS = 50\n",
    "\n",
    "#organizar en secuencias de tokens\n",
    "length = IN_WORDS + 1\n",
    "lines = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    lines.append(line)\n",
    "print('Total Sequences: %d' % len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Lenguaje\n",
    "\n",
    "La topología a entrenar es un modelo neural de lenguaje. Podemos destacar algunas de sus características:\n",
    "-\tUtiliza una representación distribuida para las palabras, de modo que diferentes palabras con significados similares tendrán una representación similar.\n",
    "-\tAprende la representación al mismo tiempo que aprende el modelo\n",
    "-\tAprende a predecir la probabilidad de la siguiente palabra utilizando el contexto de las palabras anteriores.\n",
    "\n",
    "Para esta tarea utilizaremos una capa “Embedding” para aprender la representación de palabras y una capa recurrente LSTM para aprender a predecir las palabras basado en su contexto.\n",
    "\n",
    "### Codificación de Secuencias\n",
    "La capa Word Embedding espera que las secuencias de entrada estén compuestas por números enteros. Podemos asignar cada palabra de nuestro vocabulario a un número entero único y codificar nuestras secuencias de entrada. Más adelante, cuando hagamos predicciones, podemos convertir la predicción en números y buscar sus palabras asociadas en el mismo mapeo.\n",
    "\n",
    "Para realizar esta codificación, utilizaremos la clase Tokenizer de Keras. Primero el Tokenizer debe ser entrenado en el conjunto de datos de entrenamiento, lo cual significa que encuentra todas las palabras únicas en los datos y asigna a cada una un número entero único.\n",
    "\n",
    "A continuación, podemos utilizar el Tokenizer ajustado para codificar todas las secuencias de entrenamiento, convirtiendo cada secuencia de una lista de palabras a una lista de enteros.\n",
    "\n",
    "Podemos acceder al mapeo de palabras a enteros como un atributo del diccionario llamado Word_index en el objeto Tokenizer.\n",
    "\n",
    "Necesitamos saber el tamaño del vocabulario para definir la capa Embedding. Podemos determinar el tamaño del vocabulario calculando el tamaño del diccionario codificado.\n",
    "\n",
    "A las palabras se les asigna valores desde 1 hasta el número total de palabras. La capa Embedding necesita asignar una representación vectorial para cada palabra de este vocabulario desde el índice 1 hasta el índice más grande y como la indexación de los arrays es 0, es necesario que el array tenga una longitud de: [número total de palabras + 1].\n",
    "\n",
    "Por lo tanto, al especificar el tamaño del vocabulario a la capa Embedding, lo especificamos como 1 mayor que el vocabulario real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrada y Salida de la Secuencia\n",
    "Ya hemos codificado las secuencias de texto, necesitamos separarlas en entrada (X) y salida (y). Esto lo haremos dividiendo el array.\n",
    "\n",
    "Después de que tengamos la salida, necesitamos aplicarle codificación one hot. Esto significa convertir de un número entero a un vector de ceros, uno por cada palabra del vocabulario, con un 1 para indicar la palabra especifica en el índice del valor entero de las palabras.\n",
    "\n",
    "Esto es para que el modelo aprenda a predecir la distribución de probabilidad para la siguiente palabra; la salida a partir de la cual se aprende es 0 para todas las palabras excepto la palabra que viene a continuación.\n",
    "\n",
    "Haciendo uso de la función to_categorical() de Keras, tendremos nuestra salida codificada con one hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=VOCAB_SIZE)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición y Entrenamiento del Modelo\n",
    "Ahora es tiempo de definir y entrenar el modelo de lenguaje con los datos de entrenamiento.\n",
    "\n",
    "La capa Embedding necesita conocer el tamaño del vocabulario y la longitud de la secuencia de entrada. También tiene un parámetro para especificar cuantas dimensiones se utilizarán para representar cada palabra. Es decir, el tamaño del espacio vectorial embebido. Valores comunes son 50, 100. Usaremos 50, pero este parámetro también debe ser ajustado con base a pruebas.\n",
    "\n",
    "Usaremos dos capas LSTM cada una con 100 unidades de memoria. Más unidades de memoria y una red más profunda puede mejorar los resultados, pero también se debe someter a pruebas cada topología especificada.\n",
    "\n",
    "Agregamos una capa totalmente conectada con 100 unidades para interpretar las características extraídas de las secuencias por las capas LSTM. Finalmente, la capa de salida predice la siguiente palabra como un único vector del tamaño del vocabulario con una probabilidad para cada palabra del mismo. Como función de activación usamos Softmax, con lo cual aseguramos que las salidas tienen las características de probabilidades normalizadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            163850    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3277)              330977    \n",
      "=================================================================\n",
      "Total params: 645,727\n",
      "Trainable params: 645,727\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especificamos cuantas épocas de entrenamiento utilizaremos y el tamaño del batch. En este caso entrenaremos 100 veces y con un tamaño de batch de 128.\n",
    "\n",
    "A continuación, el modelo es compilado especificando que la función de optimización deseada es entropía cruzada categórica (categorical cross entropy). Técnicamente nuestro modelo esta aprendiendo una clasificación multi clase y esta es la función de optimización adecuada para este tipo de problemas. Adicionalmente, especificaremos que utilizaremos como optimizador la implementación del algoritmo Adam y que nos interesa evaluar la precisión del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "95/95 [==============================] - 30s 319ms/step - loss: 7.0438 - accuracy: 0.0505\n",
      "Epoch 2/10\n",
      "95/95 [==============================] - 36s 383ms/step - loss: 6.5225 - accuracy: 0.0544\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - 38s 402ms/step - loss: 6.4240 - accuracy: 0.0544\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - 41s 427ms/step - loss: 6.2656 - accuracy: 0.0540\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - 42s 446ms/step - loss: 6.0871 - accuracy: 0.0563\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - 40s 422ms/step - loss: 5.9894 - accuracy: 0.0602\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - 42s 440ms/step - loss: 5.8997 - accuracy: 0.0678\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - 46s 485ms/step - loss: 5.8144 - accuracy: 0.0720\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - 48s 508ms/step - loss: 5.7399 - accuracy: 0.0721\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - 45s 478ms/step - loss: 5.6683 - accuracy: 0.0761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a82e35b288>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH = 128\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=BATCH, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el Modelo\n",
    "Una vez ajustado el modelo, vamos a guardarlo para su uso posterior. Igualmente, necesitamos tener el mapeo de palabras a enteros. Esto está en el objeto Tokenizer, lo guardaremos usando Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si en algún momento deseamos cargar un modelo entrenado previamente, utilizamos la función load_model() de Keras. En esta función solo debemos indicar el nombre el archivo .h5 que deseamos utilizar.\n",
    "\n",
    "Adicionalmente, es necesario cargar el Tokenizer que hemos utilizado previamente para codificar el documento. Este objeto lo necesitamos para mapear de número a token cuando se realice la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar un modelo guardado\n",
    "model = load_model('model-prueba.h5')\n",
    "\n",
    "# cargar el tokenizer\n",
    "tokenizer = load(open('tokenizer-prueba.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando el Modelo de Lenguaje\n",
    "Ahora que tenemos entrenado un modelo de lenguaje podemos usarlo. En este caso, lo utilizaremos para generar nuevas secuencias de texto que tengan las mimas propiedades estadísticas que el texto de origen.\n",
    "\n",
    "El primer paso para generar texto es preparar una entrada semilla. En este caso elegiremos una línea aleatoria para generar el texto.\n",
    "\n",
    "Dentro de la función generate_seq, la semilla es codificada a enteros con el mismo objeto Tokenizer que usamos cuando entrenamos el modelo. El modelo puede realizar la predicción de la siguiente palabra usando model.predict() que nos devolverá el índice de la palabra con la mayor probabilidad.\n",
    "\n",
    "Entonces, podemos buscar el índice en el mapeo del Tokenizer para obtener la palabra asociada. Adjuntamos esta palabra a la semilla y continuar el proceso de generación de texto.\n",
    "\n",
    "Debido a que la secuencia de entrada va a ser demasiado larga, podemos truncarla a la longitud deseada después de que la secuencia de entrada haya sido codificada a enteros. El truncamiento lo podemos realizar con la función pad_sequences()  de Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no le queda muela ni diente y casi sólo come migajón de pan que amasa primero en la mano hace una bola y á la boca roja allí la tiene revolviéndola una hora luego otra bola y otra masca con las encías y la barba le llega á la aguileña nariz\n",
      "\n",
      "digo que tibio igual del espacio y en la puerta cantaba un pájaro de la fuente de estopa sola platero y terrestres y el campo que trae el atajo isí eran macizos aéreos de esos carboneros que removió almorzando pasado las orejas se ve que ya la vara de oro\n"
     ]
    }
   ],
   "source": [
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "Con esta primera aproximación hemos podido generar texto automáticamente. En ocasiones parece no ser del todo coherente, se presentan errores de sintaxis, etc. Sin embargo, se debe tener en cuenta la sencilles del modelo, donde utilizamos una base de datos realmente pequeña, una topología de red neuronal bastante reducida y además no optimizada.\n",
    "\n",
    "Con fines de demostración el ejemplo planteado, junto con el modelo resultante, son aceptables. Sin lugar a dudas hay varios puntos a mejorar, desde la base de datos elegida, hasta la topología de la red neuronal utilizada. Sigamos aprendiendo y mejorando nuestros modelos, no es una tarea sencilla, ni mucho menos veloz o económica, pero es posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
