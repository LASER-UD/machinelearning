{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Texto alternativo](https://laserud.co/wp-content/uploads/2020/05/cropped-LOGOLASER-1.jpg \"Grupo LASER\")\n",
    "# Preprocesamiento de Lenguaje Natural\n",
    "El Procesamiento de Lenguaje Natural (NLP) es un área de estudio de las ciencias de la computación, la inteligencia artificial y la lingüística que se interesa por las interacciones entre las maquinas y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural (inglés, español, francés, etc.). \n",
    "\n",
    "El NLP ayuda a las maquinas a entender e interpretar diferentes idiomas, que sean capaces de descifrar las relaciones entre palabras, su significado y todos los elementos que componen un mensaje. Dentro de sus aplicaciones se pueden destacar: Asistentes virtuales, traducción automática de texto, recuperación de información, clasificación de textos, detección de sentimientos, etc.\n",
    "\n",
    "Hay muchos retos que afrontar haciendo uso de NLP pues nuestros lenguajes naturales tienen una gran riqueza, ya que pasan de generación en generación, y son difíciles de precisar con reglas explícitas. Por ejemplo, usamos palabras que dependiendo del contexto pueden significar una cosa u otra, podemos usar abreviaciones, ser sarcásticos, etc. Todos estos componentes implican retos en la aplicación de NLP.\n",
    "\n",
    "En este pequeño tutorial vamos a aprender algunas técnicas para el preprocesamiento de lenguaje natural, específicamente texto. Tenemos dos aproximaciones, la primera con un procesamiento manual y la segunda, haciendo uso de la librería para Python NLTK y Keras. Se debe tener en cuenta que el preprocesamiento aplicado a un texto depende básicamente de las necesidades del problema tratado y de las herramientas de Machine Learning utilizadas para el desarrollo.\n",
    "\n",
    "Una actividad necesaria antes de entrenar modelos de Machine Learning para Procesamiento de Lenguaje Natural (NLP) es el preprocesamiento al texto crudo. Esto puede implicar dividir el texto en palabras, darle manejo a la puntuación, a las mayúsculas y minúsculas, etc.\n",
    "\n",
    "Claramente, existe un conjunto de métodos de preparación de texto que se utilizarán dependiendo de la tarea de NLP que se desarrolle.\n",
    "\n",
    "El primer paso para la limpieza del texto es tener una idea clara de lo que se quiere conseguir, en este contexto es necesario revisar el texto para comprobar que podría ayudar.\n",
    "\n",
    "Para este ejercicio elegimos un texto simple escrito por el poeta Español Juan Ramón Jiménez en 1914. Hemos decidido utilizar esta obra para el ejemplo de hoy, pues es una obra simple, corta y el texto crudo puede ser descargado gratis desde el proyecto Gutenberg en: https://www.gutenberg.org/ebooks/39209.\n",
    "\n",
    "Al descargar el archivo de texto, podemos darnos cuenta que el archivo elegido contiene encabezado y pie de página, y no estamos interesados en esta información. Así que abrimos el archivo y borramos esta pequeña sección del archivo.\n",
    "\n",
    "Antes de empezar con la tarea de limpiado del texto, debemos tener claro lo que queremos conseguir, así que debemos revisar el texto para conocer exactamente que podría ayudar.\n",
    "\n",
    "Tomemos un momento para revisar el texto y sacar nota de algunas de las observaciones importantes:\n",
    "-\tEs un texto sin formato, así que no hay marcas que analizar\n",
    "-\tLas líneas son divididas sin un formato específico.\n",
    "-\tSe utiliza el guion (-) para indicar aclaraciones del autor o conversaciones.\n",
    "-\tSe utiliza gran variedad de signos de puntuación.\n",
    "-\tDentro del texto no se utilizan números.\n",
    "-\tHay marcadores de sección (I, II, III, etc.)\n",
    "\n",
    "\n",
    "### Lectura del Texto\n",
    "\n",
    "Vamos a empezar cargando el texto para poder a trabajar con él. El texto que hemos elegido para este ejercicio es pequeño y no tendremos problemas para cargarlo en memoria. El proceso de lectura del archivo se realiza a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://github.com/LASER-UD/machinelearning/blob/main/NLP/CleanTextNLP/platero.txt?raw=true\"\n",
    "text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización Manual\n",
    "Limpiar el texto generalmente conlleva a una lista de palabras o tokens con las cuales podemos trabajar en nuestros modelos de Machine Learning. Esto significa convertir el texto plano en una lista de palabras que guardaremos.\n",
    "\n",
    "Para realizar el proceso de división podemos basarnos en los espacios en blanco del texto original, nuevas líneas, tabulaciones. Utilizando la función Split() en Python podemos obtener el resultado esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffADVERTENCIA', 'Á', 'LOS', 'HOMBRES', 'QUE', 'LEAN', 'ESTE', 'LIBRO', 'PARA', 'NIÑOS', 'Este', 'breve', 'libro,', 'en', 'donde', 'la', 'alegría', 'y', 'la', 'pena', 'son', 'gemelas,', 'cual', 'las', 'orejas', 'de', 'Platero,', 'estaba', 'escrito', 'para...', '¡qué', 'sé', 'yo', 'para', 'quién!...', 'para', 'quien', 'escribimos', 'los', 'poetas', 'líricos...', 'Ahora', 'que', 'va', 'á', 'los', 'niños,', 'no', 'le', 'quito']\n"
     ]
    }
   ],
   "source": [
    "#dividir por espacios en blanco\n",
    "words = text.split()\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que la puntuación al final de las palabras se mantiene. Por ejemplo “gemelas,”. Algunas veces queremos solo las palabras, sin signos de puntuación. Una forma de dar solución a este problema es reemplazar toda la puntuación con nada ('').\n",
    "\n",
    "Dentro de la librería string de Python tenemos una constante con una lista de caracteres de puntuación.\n",
    "\n",
    "Obsrvmos que dentro de la lista no hay símbolos de apertura de interrogación o admiración. Esto se presenta ya que en el idioma inglés no se usan estos símbolos. Fácilmente podemos agregarlos concatenando la lista de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~¿¡\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "puntuacion = string.punctuation + '¿¡'\n",
    "print(puntuacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminando la Puntuación\n",
    "Para realizar el reemplazo de las puntuación podemos usar dos funciones, la primera translate() con la cual podemos mapear un conjunto de caracteres a otro. La segunda función es maketrans() con la cual inicialmente crearemos una tabla de mapeo.\n",
    "\n",
    "Crearemos una tabla de mapeo vacía, pero el tercer argumento de esta función corresponde a la lista de caracteres que queremos eliminar durante el proceso de traducción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffADVERTENCIA', 'Á', 'LOS', 'HOMBRES', 'QUE', 'LEAN', 'ESTE', 'LIBRO', 'PARA', 'NIÑOS', 'Este', 'breve', 'libro', 'en', 'donde', 'la', 'alegría', 'y', 'la', 'pena', 'son', 'gemelas', 'cual', 'las', 'orejas', 'de', 'Platero', 'estaba', 'escrito', 'para', 'qué', 'sé', 'yo', 'para', 'quién', 'para', 'quien', 'escribimos', 'los', 'poetas', 'líricos', 'Ahora', 'que', 'va', 'á', 'los', 'niños', 'no', 'le', 'quito']\n"
     ]
    }
   ],
   "source": [
    "table = str.maketrans('', '', puntuacion)\n",
    "stripped = [w.translate(table) for w in words]\n",
    "print(stripped[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtuvimos los resultados esperados. Se eliminaron los signos de puntuación. Pero ahora podemos ver que hay palabras con mayúsculas y otras en minúscula. El siguiente paso es normalizar las palabras dejándolas todas en minúscula, esto con el objetivo principal de reducir el vocabulario.\n",
    "\n",
    "### Normalizando a Minúsculas\n",
    "\n",
    "Con este proceso se reduce el tamaño del vocabulario, sin embargo se pierden algunas distinciones de nombres propios. Podemos convertir todas las palabras a minúscula utilizamos la función lower() para cada palabra, como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffadvertencia', 'á', 'los', 'hombres', 'que', 'lean', 'este', 'libro', 'para', 'niños', 'este', 'breve', 'libro,', 'en', 'donde', 'la', 'alegría', 'y', 'la', 'pena']\n"
     ]
    }
   ],
   "source": [
    "words = [word.lower() for word in words]\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos realizado un proceso básico manual de preparación de texto, sin embargo, hay disponibles librerías para Python que pueden hacer el proceso de preparación un poco más sencillo. A continuación, vamos a trabajar con la librería NLTK (Natural Language Toolkit).\n",
    "\n",
    "# NLTK\n",
    "NLTK es una librería de código abierto para Python, cuyo principal objetivo es trabajar con datos de lenguaje humano. Está destinado a apoyar la investigación y la enseñanza del NLP o áreas relacionadas que incluyen lingüística, recuperación de información, etc.\n",
    "\n",
    "La página web del proyecto es: https://www.nltk.org/. Allí podemos encontrar información sobre la librería, un libro guía escrito por los desarrolladores de NLTK, donde se explican categorización de textos, análisis de estructura lingüística, etc.\n",
    "\n",
    "### Dividiendo en Oraciones\n",
    "Un buen primer paso es dividir el texto en oraciones. En NLTK encontramos una función que nos ayuda con esta tarea: sent_tokenize(). Al ejecutar esta función con nuestro texto de ejemplo, podemos observar como este es dividido en párrafos, incluso se mantienen los saltos de línea del texto.\n",
    "\n",
    "Esta función es útil, ya que en algunos casos podemos necesitar dividir el texto en oraciones y luego cada una de ellas dividirla en palabras y guardar esta lista en archivos independientes por cada oración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿ADVERTENCIA Á LOS HOMBRES\n",
      "\n",
      "QUE LEAN ESTE LIBRO PARA NIÑOS\n",
      "\n",
      "\n",
      "Este breve libro, en donde la alegría y la pena\n",
      "son gemelas, cual las orejas de Platero, estaba\n",
      "escrito para... ¡qué sé yo para quién!...\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividiendo en Tokens\n",
    "\n",
    "La librería NLTK también nos brinda una función que divide el texto en palabras o tokens: word_tokenize(). Esta función utiliza los espacios en blanco y la puntuación para hacer la división."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffADVERTENCIA', 'Á', 'LOS', 'HOMBRES', 'QUE', 'LEAN', 'ESTE', 'LIBRO', 'PARA', 'NIÑOS', 'Este', 'breve', 'libro', ',', 'en', 'donde', 'la', 'alegría', 'y', 'la', 'pena', 'son', 'gemelas', ',', 'cual', 'las', 'orejas', 'de', 'Platero', ',', 'estaba', 'escrito', 'para', '...', '¡qué', 'sé', 'yo', 'para', 'quién', '!', '...', 'para', 'quien', 'escribimos', 'los', 'poetas', 'líricos', '...', 'Ahora', 'que']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el código, podemos observar que además de las palabras, los signos de puntuación también son tokens. Dependiendo de la necesidad podemos decidir filtrarlos. Un caso particular a destacar se da con los signos de admiración o pregunta al inicio de una palabra, estos no son tomados como tokens sino como parte de la palabra como tal. Esto se debe a que en inglés no se usan estos símbolos. Primero debemos eliminarlos de nuestras palabras, ejecutando la traducción que aprendimos antes en el tutorial.\n",
    "\n",
    "Podemos filtrar tokens en los cuales no estamos interesados, tales como signos de puntuación aislados. Tenemos que iterar sobre todos los tokens y mantener solo aquellos que son alfabéticos. Para esto utilizamos la función de Python isalpha().\n",
    "\n",
    "Ejecutamos el código y podemos observar como la puntuación ya está filtrada y tenemos solo la lista de palabras que queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Á', 'LOS', 'HOMBRES', 'QUE', 'LEAN', 'ESTE', 'LIBRO', 'PARA', 'NIÑOS', 'Este', 'breve', 'libro', 'en', 'donde', 'la', 'alegría', 'y', 'la', 'pena', 'son', 'gemelas', 'cual', 'las', 'orejas', 'de', 'Platero', 'estaba', 'escrito', 'para', 'qué', 'sé', 'yo', 'para', 'quién', 'para', 'quien', 'escribimos', 'los', 'poetas', 'líricos', 'Ahora', 'que', 'va', 'á', 'los', 'niños', 'no', 'le', 'quito', 'ni']\n"
     ]
    }
   ],
   "source": [
    "puntuacion = string.punctuation + '¿¡'\n",
    "table = str.maketrans('', '', puntuacion)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrando Palabras Vacias\n",
    "Las palabras comunes o palabras vacías (stop words) son aquellas palabras que no contribuyen en gran medida a las ideas de una oración. Son palabras comunes como: “el”, “la”, “es”, etc.\n",
    "\n",
    "Para algunas aplicaciones puede tener sentido eliminar las palabras vacías. Así que NLTK cuenta con una lista de palabras vacías en diversos idiomas, en este caso nos interesa el español. Podemos ver la lista de palabras como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('spanish')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar la lista es extensa. Todas las palabras están en minúscula y claramente sin puntuación. Podemos comparar estas palabras con los tokens y eliminarlas, pero debemos asegurar que nuestro texto está preparado en el mismo sentido.\n",
    "\n",
    "Vamos a hacer una pequeña lista de tareas que nos indicará los pasos para la preparación del texto:\n",
    "1.\tCargar el texto “crudo”\n",
    "2.\tDividirlo en tokens\n",
    "3.\tConvertirlo a minúsculas\n",
    "4.\tEliminar los signos de puntuación\n",
    "5.\tEliminar tokens que son palabras vacías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['á', 'hombres', 'lean', 'libro', 'niños', 'breve', 'libro', 'alegría', 'pena', 'gemelas', 'orejas', 'platero', 'escrito', 'sé', 'quién', 'escribimos', 'poetas', 'líricos', 'ahora', 'va', 'á', 'niños', 'quito', 'pongo', 'coma', 'bien', 'dondequiera', 'niños', 'dice', 'nóvalis', 'existe', 'edad', 'oro', 'pues', 'edad', 'oro', 'isla', 'espiritual', 'caída', 'cielo', 'anda', 'corazón', 'poeta', 'encuentra', 'allí', 'tan', 'á', 'gusto', 'mejor', 'deseo']\n"
     ]
    }
   ],
   "source": [
    "#Convertimos las palabras a minusculas\n",
    "words = [w.lower() for w in words]\n",
    "\n",
    "#Filtramos las palabras comunes\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras Raíz\n",
    "\n",
    "El steamming es un proceso para reducir una palabra a su raíz o base. Algunas aplicaciones, como la clasificación de documentos, se pueden beneficiar de la aplicación del steamming. Por una parte, se reduce el vocabulario, mientras que se enfoca en la idea o sentimiento de un texto más que un significado profundo.\n",
    "\n",
    "Hay diversos algoritmos de stemming, aunque un método popular y confiable es el algoritmo Porter Stemming. Este algoritmo está disponible en la librería NLTK mediante la clase PorterSteammer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffadvertencia', 'á', 'lo', 'hombr', 'que', 'lean', 'est', 'libro', 'para', 'niño', 'est', 'breve', 'libro', ',', 'en', 'dond', 'la', 'alegría', 'y', 'la', 'pena', 'son', 'gemela', ',', 'cual', 'la', 'oreja', 'de', 'platero', ',', 'estaba', 'escrito', 'para', '...', '¡qué', 'sé', 'yo', 'para', 'quién', '!', '...', 'para', 'quien', 'escribimo', 'lo', 'poeta', 'lírico', '...', 'ahora', 'que']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el ejemplo podemos observar que las palabras se han reducido a su base; además, las palabras quedaron normalizadas en minúscula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento con Keras\n",
    "Usando la librería Keras podemos realizar la preparación de los datos crudos para poder usarlos con topologías de Machine Learning.\n",
    "\n",
    "### Función text_to_word_sequence\n",
    "\n",
    "Primero observaremos como podemos dividir texto en palabras o tokens usando la función text_to_word_sequence(). Esta función realiza automáticamente tres procesos:\n",
    "-\tDivide el texto basándose en los espacios\n",
    "-\tFiltra la puntuación, pero nuevamente los signos “¿¡” no son considerados en el idioma inglés\n",
    "-\tConvierte el texto a minúsculas\n",
    "\n",
    "Podemos ver un ejemplo de funcionamiento a continuación, con una cadena de texto simple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['esto', 'es', 'una', 'prueba', 'cómo', 'vas']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import string\n",
    "\n",
    "filters = string.punctuation+ '¿¡'\n",
    "text = \"¡Esto es una prueba!¿Cómo vas?\"\n",
    "result = text_to_word_sequence(text, filters=filters, lower=True, split=' ')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, algunos signos de puntuación no son tomados en cuenta para el filtrado originalmente. Por lo que realizamos un proceso adicional para eliminar estos signos que no nos interesan, especificando nosotros mismos los caracteres a filtrar.\n",
    "\n",
    "### Codificación con one_hot\n",
    "Keras nos brinda una función que podemos utilizar para tokenizar y codificar con enteros un documento en un solo paso. Esta función retorna una versión codificada con enteros del documento. El uso de una función hash significa que pueden presentarse colisiones y que no se asignarán valores enteros únicos a todas las palabras.\n",
    "\n",
    "Esta función convertirá el texto a minúsculas, filtrará la puntuación y dividirá el texto basándose en los espacios.\n",
    "Para utilizar esta función, debemos especificar además del texto, el tamaño del vocabulario o el total de palabras. Este debe ser igual al número total de palabras en el documento o mayor si tu pretendes codificar otros documentos que puedan contener palabras adicionales. El tamaño del vocabulario define el espacio de hashing desde las cuales las palabras son divididas. Idealmente, este debe ser más grande que el vocabulario por algún porcentaje (tal vez 25%) para minimizar el número de colisiones. Por defecto, se utiliza la función hash aunque podemos alternar funciones hash cuando llamamos la función hashing_trick() directamente.\n",
    "\n",
    "Para conocer el número total de palabras en el texto, podemos usar la función text_to_word_sequence(), y luego localizar solo las palabras únicas en el documento. El número de palabras totales puede ser usado para estimar el tamaño del vocabulario para un documento.\n",
    "\n",
    "Vamos a ver un ejemplo, donde aumentamos el tamaño del vocabulario en un 30% para minimizar colisiones cuando se codifiquen las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[3, 6, 4, 7, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "filters = string.punctuation+ '¿¡'\n",
    "text = \"¡Esto es una prueba!¿Cómo vas?\"\n",
    "\n",
    "vocab_size = len(set(text_to_word_sequence(text, filters=filters)))\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "result = one_hot(text, round(vocab_size*1.3), filters=filters)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación Con hashing_trick\n",
    "\n",
    "Una limitación de las codificaciones en base entera y de conteo es que se debe mantener un vocabulario de palabras y su asignación a enteros.\n",
    "\n",
    "Una alternativa a este enfoque es utilizar una función hash unidireccional para convertir las palabras a enteros. Esto evita la necesidad de seguir un vocabulario, lo cual es más rápido y requiere menos memoria.\n",
    "\n",
    "Keras nos brinda la función hashing_trick() que toqueniza y posteriormente codifica con enteros el documento, tal como la función one_hot(). Esta función proporciona mayor flexibilidad, permitiéndote especificar la función de hash. Por defecto la función de hash es “hash”, sin embargo, se puede elegir la función “md5” o incluso una función propia.\n",
    "\n",
    "A continuación, se muestra un ejemplo del uso de la función hashing_trick()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 7, 5, 4, 2]\n",
      "[2, 6, 4, 7, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "filters = string.punctuation+ '¿¡'\n",
    "text = \"¡Esto es una prueba!¿Cómo vas?\"\n",
    "\n",
    "vocab_size = len(set(text_to_word_sequence(text, filters=filters)))\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)\n",
    "\n",
    "#si no especificamos el parámetro hash_function, por defecto toma \"hash\"\n",
    "result = hashing_trick(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los desarrolladores nos advierten que 'hash' no es una función hash estable, por lo que no es consistente a través de diferentes ejecuciones, mientras que 'md5' es una función hash estable.\n",
    "\n",
    "### Tokenizer API\n",
    "\n",
    "Adicional a las funciones que hemos tratado anteriormente, Keras nos brinda una API más sofisticada para procesar texto que puede ser ajustada y reusada para preparar múltiples documentos. Esta puede ser la aproximación preferida para proyectos grandes.\n",
    "\n",
    "Keras proporciona la clase Tokenizer para preparar documentos de texto para proyectos de Deep Learning. El tokenizer debe ser construido y entonces ser ajustado en documentos de texto crudo o documentos de texto codificado con enteros.\n",
    "Una vez se ajusta el Tokenizer, este tiene cuatro atributos que podemos utilizar para conocer que se ha aprendido del documento.\n",
    "-\tWord_counts: un diccionario de palabras y sus conteos.\n",
    "-\tWord_docs: un diccionario de palabras y en cuantos documentos aparece cada una.\n",
    "-\tWord_index: un diccionario de palabras y sus enteros únicos asignados.\n",
    "-\tDocument_count: un recuento del número total de documentos que se utilizaron para ajustar el Tokenizer.\n",
    "Una vez el Tokenizer es ajustado a los datos de entrenamiento, este puede ser utilizado para codificar documentos en los conjuntos de prueba o validación.\n",
    "\n",
    "La función text_to_matrix() en el Tokenizer, puede ser utilizada para crear un vector por documento proporcionado por entrada. La longitud del vector es el tamaño total del vocabulario.\n",
    "\n",
    "Esta función proporciona un conjunto de esquemas de codificación de texto del modelo Bag of Words estándar que puede proporcionarse mediante un argumento a la función.\n",
    "\n",
    "Los modos disponibles son:\n",
    "-\t‘binary’: donde se presenta o no cada palabra en el documento. Esta es el modo por defecto.\n",
    "-\t‘count’: El conteo de cada palabra en el documento.\n",
    "-\t‘tfidf’: La puntuación Text Frequency-Inverse Document Frequency (TF-IDF) de cada palabra del documento.\n",
    "-\t‘freq’: La frecuencia de cada palabra como proporción de palabras dentro de cada documento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('esto', 1), ('es', 2), ('una', 2), ('prueba', 2), ('cómo', 1)])\n",
      "1\n",
      "{'es': 1, 'una': 2, 'prueba': 3, 'esto': 4, 'cómo': 5}\n",
      "defaultdict(<class 'int'>, {'prueba': 1, 'esto': 1, 'cómo': 1, 'una': 1, 'es': 1})\n",
      "[[0. 2. 2. 2. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "filters = string.punctuation+ '¿¡'\n",
    "\n",
    "#definimos el texto como lista\n",
    "text = [\"¡Esto es una prueba!¿Cómo es una prueba?\"]\n",
    "t = Tokenizer(char_level=False, filters=filters)\n",
    "t.fit_on_texts(text)\n",
    "\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "\n",
    "encoded_docs = t.texts_to_matrix(text, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "En este tutorial aprendimos algunos métodos básicos importantes para la preparación de datos para aplicaciones de NLP. En las próximas entradas vamos a aplicar estos métodos de preprocesamiento para ejemplificar mejor el funcionamiento del NLP usando Machine Learning. Por el momento, sabemos que dependiendo de las necesidades específicas del problema aplicaremos unas u otras técnicas de preprocesamiento, también entendemos que empezamos explicando técnicas sencillas, pero de mucha utilidad práctica. En el tutorial se fue avanzando desde el procesamiento manual, hasta el procesamiento con librerías importantes en el área de NLP y Machine Learning, como NLTK y Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
