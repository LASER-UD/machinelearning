{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT SUMMARIZATION\n",
    "El resumen de textos es el problema de crear una síntesis corta, precisa y fluida de textos más extensos. Los métodos de síntesis automática son necesarios hoy en día para dar tratamiento a los datos de textos siempre crecientes en internet, por una parte, para ayudar a descubrir información relevante, como para hacer el consumo de información más rápido.\n",
    "\n",
    "Tenemos disponibles un gran volumen de datos textuales, y está creciendo cada día más. Si pensamos en internet, cada día se publican miles de noticias, artículos, blogs, etc. Estos son datos no estructurados y lo mejor que podemos hacer para navegar y encontrar lo que necesitamos es utilizar la búsqueda y hojear los resultados.\n",
    "\n",
    "Entonces, existe una gran necesidad de reducir muchos de estos textos, enfocándose en los detalles sobresalientes. Esto permite una navegación más efectiva y es una forma más eficiente de establecer cuando un texto nos es útil o no, si contienen la información que estamos buscando o no. Claramente, no es sostenible crear síntesis de todos estos textos de forma manual, necesitamos de métodos automáticos.\n",
    "\n",
    "“La información textual en forma de documentos digitales se acumula rápidamente hasta alcanzar enormes cantidades de datos. La mayor parte de este gran volumen de documentos no está estructurada: no tiene límites y no se ha organizado en bases de datos tradicionales. El tratamiento de los documentos es, por tanto, una tarea superficial, sobre todo por la falta de normas.” Automatic Text Summarization, 2014.\n",
    "\n",
    "En este mismo libro, los autores exponen 6 razones por las cuales necesitamos de herramientas de resumen de texto automáticas:\n",
    "\n",
    "-\tLos resúmenes reducen el tiempo de lectura.\n",
    "-\tAl buscar documentos, los resúmenes facilitan el proceso de selección.\n",
    "-\tEl resumen automático mejora la eficacia de la indexación.\n",
    "-\tLos algoritmos de resumen automático son menos parciales que los resumidores humanos.\n",
    "-\tLos resúmenes personalizados son útiles en los sistemas de respuesta a preguntas, ya que proporcionan información personalizada.\n",
    "-\tEl uso de sistemas de resumen automático o semiautomático permite a los servicios comerciales de resúmenes aumentar el número de textos que pueden procesar.\n",
    "\n",
    "Como ya hemos mencionado anteriormente, los Resúmenes de Texto Automáticos (Automatic Text Summarization) son el proceso de crear una versión corta y coherente de un texto más extenso. Es el proceso de extraer la información más importante de una fuente o varias fuentes, para producir una versión abreviada para un usuario o tarea particular.\n",
    "\n",
    "Las tareas de resumir textos normalmente son bien ejecutadas por los humanos, el proceso de síntesis generalmente involucra, primero entender el documento para luego extraer la idea principal y los detalles importantes en la síntesis del documento. En los procesos automáticos, tal como se entiende, la idea es generar síntesis de textos extensos de forma automática, con la salvedad de que estas deben ser coherentes y que puedan ser comparables a las síntesis obtenidas por humanos.\n",
    "\n",
    "Para la generación automática de síntesis, no es suficiente con extraer palabras y frases que capturen la idea principal de un texto. Más bien, la síntesis debe ser precisa y debe poderse leer de forma fluida, como si se tratase de un nuevo documento aislado.\n",
    "\n",
    "Como hemos mencionado antes, hay muchas razones para sintetizar textos. En su libro de 1999 “Advances in Automatic Text Summarization” los autores proveen una lista útil de aplicaciones de la vida real para síntesis de textos:\n",
    "\n",
    "-\ttitulares (de todo el mundo)\n",
    "-\tesquemas (apuntes para los alumnos)\n",
    "-\tactas (de una reunión)\n",
    "-\tpreestrenos (de películas)\n",
    "-\tsinopsis (de telenovelas)\n",
    "-\treseñas (de un libro, CD, película, etc.)\n",
    "-\tresúmenes (guía de televisión)\n",
    "-\tbiografías (currículos, obituarios)\n",
    "-\tresúmenes (Shakespeare para niños)\n",
    "-\tboletines (previsiones meteorológicas o informes bursátiles)\n",
    "-\tfragmentos de sonido (políticos sobre un tema de actualidad)\n",
    "-\thistorias (cronologías de acontecimientos destacados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo Sintetizar Textos?\n",
    "Hay principalmente dos métodos para sintetizar texto, métodos extractivos y métodos abstractivos.\n",
    "\n",
    "La sinterización mediante métodos extractivos involucra la selección de frases y oraciones del texto fuente, para luego sintetizarlas en un nuevo documento. Estas técnicas involucran la clasificación de frases relevantes con el objetivo de seleccionar solo aquellas que involucran la idea principal del texto.\n",
    "\n",
    "Los métodos abstractivos involucran generar enteramente nuevas frases y oraciones que capturan la idea principal del texto fuente. Esta es una alternativa más desafiante, pero también es el método naturalmente usado por los humanos. En este método se interpreta y examina el documento con el objetivo de generar un nuevo texto más corto que contenga la información más importante del texto original.\n",
    "\n",
    "## ¿Qué es un Modelado Secuencia a Secuencia (Seq2Seq)?\n",
    "Seq2Seq es un modelo que toma un flujo de oraciones como entrada y otro flujo de oraciones como salida. Esto puede ser visto en Neural Machine Translation donde las oraciones de entrada están en un lenguaje y las oraciones de salida son versiones traducidas de ese lenguaje. El codificador y decodificador son las dos principales técnicas usadas en el modelado seq2seq.\n",
    "\n",
    "Modelo codificador: es utilizado para codificar o transformar las oraciones de entrada y genera retroalimentación después de cada paso. Esta retroalimentación puede ser un estado interno, es decir, estados ocultos o celdas de estado si usamos las capas LSTM. Los modelos codificadores capturan la información vital de las frases de entrada manteniendo el contexto en todo momento.\n",
    "\n",
    "Modelo decodificador: el modelo decodificador es usado para decodificar o predecir las oraciones objetivo palabra por palabra. Los datos de entrada del decodificador toman la entrada de las frases objetivo y predicen la siguiente palabra que se introduce en la siguiente capa para la predicción. \"start\" (inicio de frases objetivo) y ‘<end>’ (final de la frase objetivo) son las dos palabras que ayudan al modelo a conocer cuál será la variable inicial para predecir la siguiente palabra y la variable final para conocer el final de la oración. Mientras entrenamos el modelo, primero proporcionamos la palabra \"start\", el modelo predice entonces la siguiente palabra que es el dato objetivo del decodificador. Esta palabra se introduce como dato de entrada en el siguiente paso de tiempo para obtener la predicción de la siguiente palabra.\n",
    "\n",
    "Nuestros datos de entrada comenzarán en \"start\" y el objetivo predecirá la siguiente palabra con la ayuda de los datos de entrada en cada paso de tiempo. Nuestros datos de entrada no contienen la última palabra, ya que nuestro dato objetivo en el último paso de tiempo es \"end\", lo que nos indica que hemos llegado al final de nuestra frase y detenemos la iteración. De la misma manera nuestros datos objetivo estarán un paso adelante ya que la primera palabra \"inicio\" es proporcionada por los datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecanismo de Atención\n",
    "Vamos a tomar un ejemplo para entender el mecanismo de atención. A continuación, tenemos la entrada de texto (revisión) y texto objetivo (resumen).\n",
    "\n",
    "Texto de entrada: Ahora que he aprendido sobre el aprendizaje automático, me gustaría trabajar en algunos proyectos. ¿Puede alguien recomendar la mejor fuente para proyectos de aprendizaje automático?\n",
    "\n",
    "Texto objetivo: Necesito saber cuál es la mejor fuente de proyectos de aprendizaje automático.\n",
    "\n",
    "Con el mecanismo de atención, en lugar de centrarnos en todas las palabras, que es muy difícil de recordar, sólo nos centraremos en palabras específicas para la predicción. En nuestro ejemplo, sólo nos centraremos en palabras como \"fuente\", \"aprendizaje automático\" y \"proyectos\" para predecir el texto de destino.\n",
    "\n",
    "Hay dos mecanismos de atención principalmente:\n",
    "\n",
    "a)\tAtención global: En este mecanismo los estados ocultos de cada paso de tiempo del modelo codificador se utilizan para generar el vector de contexto.\n",
    "\n",
    "b)\tAtención Local: Algunos de los estados ocultos del modelo del codificador se utiliza para generar el vector de contexto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Seq2Seq\n",
    "\n",
    "En este proyecto utilizaremos modelos de secuencia muchos a muchos usando la técnica de síntesis abstractiva para crear modelos que realicen el resumen de las reseñas de productos de Amazon. El modelo será entrenado y probado con el primer 1000000 de registros del conjunto de datos. Usando el mecanismo de atención nos enfocaremos en palabras clave especificas mientras se mantiene el contexto de la oración.\n",
    "\n",
    "Primero vamos a importar las librerías que necesitaremos para el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from statistics import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a leer el archivo .csv, en este caso solo utilizaremos el primer 1000000 de registros. Nuestra entrada será la columna ‘Text’ que es la columna de reseña y el objetivo será la columna ‘Summary’. Eliminaremos los registros duplicados y los valores N/A de nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4b99a6f4b723>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reviews.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#eliminamos duplicados y valores nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Reviews.csv\",nrows=100000)\n",
    "#eliminamos duplicados y valores nan\n",
    "df.drop_duplicates(subset=['Text'],inplace=True)\n",
    "df.dropna(axis=0,inplace=True)\n",
    "input_data = df.loc[:,'Text']\n",
    "target_data = df.loc[:,'Summary']\n",
    "target_data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "Los textos del mundo real están incompletos y no se pueden enviar directamente al modelo, pues provocaría ciertos errores. Por lo tanto, limpiamos todos nuestros textos y los convertimos en una forma presentable para las tareas de predicción. Así que, en primer lugar, inicializaremos todas las variables y métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_words=[]\n",
    "target_words=[]\n",
    "contractions= pickle.load(open(\"contractions.pkl\",\"rb\"))['contractions']\n",
    "#definimos las palabras vacias en ingles y el Lancaster Stemmer\n",
    "stop_words=set(stopwords.words('english'))\n",
    "stemm=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos de los textos están en formato html y contienen etiquetas html, así que primero analizaremos este texto y eliminaremos todas las etiquetas html utilizando la biblioteca BeautifulSoup. Posteriormente, tokenizaremos nuestro texto en palabras y verificaremos lo siguiente:\n",
    "1.\tContiene números enteros\n",
    "2.\tTiene menos de 3 caracteres\n",
    "3.\tSon palabras comunes (stop words)\n",
    "\n",
    "Si al menos una de estas condiciones se cumple, vamos a eliminar esta palabra de la lista de palabras de entrada u objetivo.\n",
    "\n",
    "En inglés se utilizan palabras con contracciones en nuestros textos de entrada y objetivo. Las contracciones son las combinaciones de dos palabras, acortadas utilizando el apóstrofe o suprimiendo letras, por ejemplo, \"haven't\" se acorta por \"have not\". Expandiremos este tipo de palabras utilizando el archivo \"contractions.pkl\", que contiene un diccionario con claves como palabras acortadas y valores como palabras expandidas. Además, reduciremos todas las palabras introducidas a su raíz. Recordemos que este proceso se conoce como Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(texts,src):\n",
    "  #eliminamos las etiquetas html\n",
    "  texts = BeautifulSoup(texts, \"lxml\").text\n",
    "  #tokenizamos en palabras\n",
    "  words=word_tokenize(texts.lower())\n",
    "  #filtramos palabras que contengan enteros o su longitud sea <= a 3\n",
    "  words= list(filter(lambda w:(w.isalpha() and len(w)>=3),words))\n",
    "  #expandimos las palabras con contracciones\n",
    "  words= [contractions[w] if w in contractions else w for w in words ]\n",
    "  #llevamos las palabras a la raiz y filtramos las palabras vacias\n",
    "  if src==\"inputs\":\n",
    "    words= [stemm.stem(w) for w in words if w not in stop_words]\n",
    "  else:\n",
    "    words= [w for w in words if w not in stop_words]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadiremos ‘sos’ al inicio y ‘eos’ al final del texto objetivo para advertirle al modelo que este es el inicio y final de las oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasamos los registros de entrada y los registros objetivo\n",
    "for in_txt,tr_txt in zip(input_data,target_data):\n",
    "  in_words= clean(in_txt,\"inputs\")\n",
    "  input_texts+= [' '.join(in_words)]\n",
    "  input_words+= in_words\n",
    "  #agregamos 'sos' al principio y 'eos' al final del texto\n",
    "  tr_words= clean(\"sos \"+tr_txt+\" eos\",\"target\")\n",
    "  target_texts+= [' '.join(tr_words)]\n",
    "  target_words+= tr_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de limpiar las oraciones filtraremos las palabras duplicadas y las organizaremos de forma acorde. También almacenaremos el número total de palabras de entrada y objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos solo palabras unicas de la lista de entrada y objtivo\n",
    "input_words = sorted(list(set(input_words)))\n",
    "target_words = sorted(list(set(target_words)))\n",
    "num_in_words = len(input_words) #numero total de palabras de entrada\n",
    "num_tr_words = len(target_words) #numero total de palabras objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos la longitud de los textos de entrada y objetivo que aparecen con más frecuencia\n",
    "max_in_len = mode([len(i) for i in input_texts])\n",
    "max_tr_len = mode([len(i) for i in target_texts])\n",
    "\n",
    "print(\"Numero de palabras de entrada : \",num_in_words)\n",
    "print(\"Numero de palabras objetivo : \",num_tr_words)\n",
    "print(\"Maxima longitud de entrada : \",max_in_len)\n",
    "print(\"Maxima longitud de objetivo : \",max_tr_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividiremos la base de datos en un subconjunto de entrenamiento y pruebas. En este caso vamos a utilizar una división 80:20, 80% del total de los datos se utilizarán para entrenar el modelo y 20% para probarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividimos las entradas y objetivos en 80:20\n",
    "x_train,x_test,y_train,y_test=train_test_split(input_texts,target_texts,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertiremos nuestras palabras en una secuencia de números enteros utilizando vectorización.\n",
    "Por ejemplo:\n",
    "\n",
    "L = [‘como’, ‘hola’, ‘que estas haciendo’]\n",
    "\n",
    "Tokenizamos todos los elementos de la lista ‘L’ y luego hacemos un diccionario con los tokens como clave y un contador como valor. Obtendremos un diccionario como el siguiente:\n",
    "\n",
    "D = {‘como’:1, ‘hola’:2, ‘que’:3, ‘estas’:4, ‘haciendo’:5}\n",
    "\n",
    "Una vez hemos ajustado nuestros datos, vamos a transformar la siguiente lista ‘T’ dentro de una secuencia de enteros usando nuestro tokenizador:\n",
    "\n",
    "T = [‘hola, como estas’, ‘que estas haciendo’]\n",
    "\n",
    "Transformado: T = [[2, 1, 4], [3, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entranamos el tokenizador\n",
    "in_tokenizer = Tokenizer()\n",
    "in_tokenizer.fit_on_texts(x_train)\n",
    "tr_tokenizer = Tokenizer()\n",
    "tr_tokenizer.fit_on_texts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos el texto a secuencias de enteros\n",
    "#el entero sera el indice de esa palabra\n",
    "x_train= in_tokenizer.texts_to_sequences(x_train) \n",
    "y_train= tr_tokenizer.texts_to_sequences(y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de convertir la secuencia a enteros, también haremos que los textos de entrada y objetivo tengan la misma longitud para nuestro modelo. Esto lo conseguiremos tomando la longitud máxima de la secuencia de entrada y también repetiremos para la secuencia objetivo. Posteriormente, rellenamos las matrices con 0 para las secuencias más cortas que la longitud máxima establecida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si el arreglo tiene menor longitud que la\n",
    "#maxima longitud de entrada y objetivo agregamos ceros\n",
    "en_in_data= pad_sequences(x_train,  maxlen=max_in_len, padding='post') \n",
    "dec_data= pad_sequences(y_train,  maxlen=max_tr_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos del codificador se rellenarán con ‘x_train’ y los datos de entrada del decodificador se rellenarán con ‘y_train’, pero sin incluir la última palabra ‘eos’. Los datos objetivo del decodificador serán los mismos que los datos de entrada del decodificador, pero están un paso adelante, ya que no incluirán la palabra inicial de nuestra frase objetivo ‘sos’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#los datos de entrada del decodificador no incluiran la palabra 'eos'\n",
    "dec_in_data = dec_data[:,:-1]\n",
    "#los datos objetivo del decodificador no \n",
    "#los datos de destino del decodificador estarán un paso por delante,\n",
    "#ya que no incluirán la primera palabra, es decir, 'sos'\n",
    "dec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del Modelo\n",
    "Para el modelo estamos usando un stack de 3 capas LSTM, esto puede ser suficiente para ciertos requerimientos. Vamos a entender nuestro modelo codificador y decodificador.\n",
    "\n",
    "Codificador: Vamos a inicializar el tensor de entrada usando el objeto ‘Input’. El tamaño esperado del batch será el máximo tamaño de entrada (74). Entonces crearemos una capa ‘Embedding’ la cual tendrá el número total de palabras de entrada como primer argumento y una dimensión latente (oculta) de 500.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session() \n",
    "latent_dim = 500\n",
    "\n",
    "#creamos el objeto de entrada del número total de palabras de entrada\n",
    "en_inputs = Input(shape=(max_in_len,)) \n",
    "en_embedding = Embedding(num_in_words+1, latent_dim)(en_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM: Ahora vamos a crear el stack de 3 capas LSTM. La primera capa tendrá como entrada el codificador y así crear una secuencia continua de capas LSTM.\n",
    "\n",
    "La capa LSTM capturará toda la información contextual presente en la secuencia de entrada. Devolveremos la salida del estado oculto y también los estados, es decir, el estado oculto y el estado de la unidad, tras la ejecución de cada capa LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos 3 capas LSTM en stack con el tamaño de la dimension oculta\n",
    "#LSTM 1\n",
    "en_lstm1= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "en_outputs1, state_h1, state_c1= en_lstm1(en_embedding) \n",
    "\n",
    "#LSTM2\n",
    "en_lstm2= LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "en_outputs2, state_h2, state_c2= en_lstm2(en_outputs1) \n",
    "\n",
    "#LSTM3\n",
    "en_lstm3= LSTM(latent_dim,return_sequences=True,return_state=True)\n",
    "en_outputs3 , state_h3 , state_c3= en_lstm3(en_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estados del codificador\n",
    "en_states= [state_h3, state_c3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodificador: Como en el codificador inicializaremos el tensor de entrada del decodificador y entonces pasar este a una capa LSTM. Aquí, el decodificador también tendrá el estado oculto inicial donde pasaremos los valores del estado oculto y el estado de la unidad que hemos obtenido de la capa LSTM del codificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificador\n",
    "dec_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(num_tr_words+1, latent_dim) \n",
    "dec_embedding = dec_emb_layer(dec_inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicializamos la capa LSTM del decodificador con los estados de salida del codificador\n",
    "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "dec_outputs, *_ = dec_lstm(dec_embedding,initial_state=en_states) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capa de atención: Pasaremos las salidas del codificador y del decodificador a la capa de atención y luego concatenaremos las salidas de la capa de atención con las del decodificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capa de atencion\n",
    "attention =Attention()\n",
    "attn_out = attention([dec_outputs,en_outputs3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenamos la salida de la capa de atencion con la salida del decodificador\n",
    "merge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos nuestra capa totalmente conectada, que es la capa de salida de nuestro modelo. Tendrá la dimensión de salida del número total de palabras objetivo y usaremos una función de activación softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capa totalmente conectada (salida)\n",
    "dec_dense = Dense(num_tr_words+1, activation='softmax') \n",
    "dec_outputs = dec_dense(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, inicializaremos nuestra clase Modelo con los datos de entrada y salida de las capas codificadoras y decodificadoras. Podemos trazar las capas del modelo y también obtener el resumen de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos el modelo y vemos un resumen\n",
    "model = Model([en_inputs, dec_inputs], dec_outputs) \n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasaremos los datos y entrenaremos nuestro modelo con un tamaño de batch de 512, 10 epochs y utilizaremos el optimizador 'RMSprop' para entrenar nuestro modelo. Se puede aumentar o disminuir el número de epochs pero hay que tener cuidado con la pérdida de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compilamos y entrenamos el modelo\n",
    "model.compile( \n",
    "    optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) \n",
    "model.fit( \n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data, \n",
    "    batch_size=512, \n",
    "    epochs=10, \n",
    "    validation_split=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de que nuestro modelo sea entrenado obtendremos un directorio como 's2s/' con 'saved_model.pb' que incluye el optimizador, las pérdidas y las métricas de nuestro modelo. Los pesos se guardan en el directorio variables/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos el modelo\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELO DE INFERENCIA\n",
    "Utilizaremos el modelo guardado para crear una arquitectura de inferencia para el modelo de codificador y decodificador. El modelo de inferencia se utiliza para probar las nuevas frases para las que no se conoce la secuencia objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder inference\n",
    "latent_dim=500\n",
    "#cargamos el modelo\n",
    "model = models.load_model(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificador de inferencia: La entrada para el modelo de codificador de inferencia será la capa 0, es decir, el objeto de entrada que hemos creado (puedes comprobarlo en el resumen anterior y en el gráfico del modelo) y la salida será la salida del último LSTM que es la capa 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contruimos el modelo del codificador desde la salida de la capa 6, ultima LSTM\n",
    "en_outputs,state_h_enc,state_c_enc = model.layers[6].output\n",
    "en_states=[state_h_enc,state_c_enc]\n",
    "#agregamos la entrada y el estado desde la capa\n",
    "en_model = Model(model.input[0],[en_outputs]+en_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodificador de inferencia: Al igual que el modelo de inferencia del codificador, obtendremos las capas de entrada, embedded y LSTM del modelo guardado. Inicializaremos la entrada oculta del decodificador y los otros dos estados con las dimensiones latentes (ocultas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder inference\n",
    "#Creamos un objeto de entrada para el estado oculto y de celda\n",
    "#para el tamaño del decodificador de la capa con dimension oculta o latente\n",
    "dec_state_input_h = Input(shape=(latent_dim,))\n",
    "dec_state_input_c = Input(shape=(latent_dim,))\n",
    "dec_hidden_state_input = Input(shape=(max_in_len,latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos las capa embedding y de entrada del modelo\n",
    "dec_inputs = model.input[1]\n",
    "dec_emb_layer = model.layers[5]\n",
    "dec_lstm = model.layers[7]\n",
    "dec_embedding= dec_emb_layer(dec_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregamos entrada e inicializamos la capa LSTM con los estados LSTM del codificador\n",
    "dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, initial_state=[dec_state_input_h,dec_state_input_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atención de inferencia: En nuestro caso, la octava capa es la capa de atención. Lo obtendremos y pasaremos la salida del decodificador de inferencia con la entrada del estado oculto que hemos inicializado anteriormente. Luego concatenaremos la salida del decodificador con la salida de la capa de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention layer\n",
    "attention = model.layers[8]\n",
    "attn_out2 = attention([dec_outputs2,dec_hidden_state_input])\n",
    "\n",
    "merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y lo mismo para la capa totalmente conectada (capa de salida) que es la décima capa de nuestro modelo guardado. Inicializar la clase Modelo de Inferencia con los datos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense layer\n",
    "dec_dense = model.layers[10]\n",
    "dec_outputs2 = dec_dense(merge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos el modelo\n",
    "dec_model = Model(\n",
    "[dec_inputs] + [dec_hidden_state_input,dec_state_input_h,dec_state_input_c],\n",
    "[dec_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos un diccionario con una clave como índice y un valor como palabras\n",
    "reverse_target_word_index = tr_tokenizer.index_word\n",
    "reverse_source_word_index = in_tokenizer.index_word\n",
    "target_word_index = tr_tokenizer.word_index\n",
    "reverse_target_word_index[0]=' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codificar la secuencia de entrada como vectores de estado. Crear una matriz vacía de la secuencia de destino y generar la palabra de inicio, es decir, \"sos\" en nuestro caso, para cada par. Utilizar este valor de estado junto con la secuencia de entrada para predecir el índice de salida. Utilizar el índice de la palabra de destino inverso para obtener la palabra del índice de salida y añadirla a la secuencia decodificada.\n",
    "\n",
    "Asignar el índice de nuestra palabra a la secuencia de destino para que en la siguiente iteración nuestra secuencia de destino tenga un vector de la palabra anterior. Iterar hasta que nuestra palabra sea igual a la última palabra, es decir, \"eos\" en nuestro caso o la longitud máxima del texto de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    #obtenemos la salida y los estados del codificador pasando la secuencia de entrada\n",
    "    en_out, en_h, en_c= en_model.predict(input_seq)\n",
    "\n",
    "    #secuencia objetivo con palabra inicial 'sos'\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_word_index['sos']\n",
    "\n",
    "    #si la iteracion llega al final del texto entonces se parara la iteracion\n",
    "    stop_condition = False\n",
    "    #añadimos cada palabra prevista en la frase decodificada\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: \n",
    "        #obtenemos la salida predecida, el estado oculto y de la unidad\n",
    "        output_words, dec_h, dec_c= dec_model.predict([target_seq] + [en_out,en_h, en_c])\n",
    "        \n",
    "        #obtenemos el indice y del diccionario obtenemos la palabra para ese indice\n",
    "        word_index = np.argmax(output_words[0, -1, :])\n",
    "        text_word = reverse_target_word_index[word_index]\n",
    "        decoded_sentence += text_word +\" \"\n",
    "\n",
    "        #condiciones de salida: se alcanza la maxima longiutd\n",
    "        #o encuentra una palbra de parada o la ultima palabra\n",
    "        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
    "          stop_condition = True\n",
    "        \n",
    "        #actualizar la secuencia de objetivo al índice de la palabra actual\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = word_index\n",
    "        en_h, en_c = dec_h, dec_c\n",
    "    \n",
    "    #devuelve la secuencia decodificada\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando el modelo\n",
    "Finalmente, hemos realizado todos los procesos y ahora podemos predecir el resumen para la revisión de la entrada.\n",
    "\n",
    "Por ejemplo podríamos ingresar la siguiente reseña:\n",
    "\n",
    "\"I am very satisfied, product is as advertised, I use it on cereal, with raw vinegar, and as a general sweetner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_review = input(\"Enter : \")\n",
    "print(\"Review :\",inp_review)\n",
    "\n",
    "inp_review = clean(inp_review,\"inputs\")\n",
    "inp_review = ' '.join(inp_review)\n",
    "inp_x= in_tokenizer.texts_to_sequences([inp_review]) \n",
    "inp_x= pad_sequences(inp_x,  maxlen=max_in_len, padding='post')\n",
    "\n",
    "summary=decode_sequence(inp_x.reshape(1,max_in_len))\n",
    "if 'eos' in summary :\n",
    "  summary=summary.replace('eos','')\n",
    "print(\"\\nPredicted summary:\",summary);print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "En este tutorial aprendimos una de las tantas tareas que podemos realizar utilizando el procesamiento de lenguaje natural, la síntesis de textos. Para ejecutar la síntesis automática de textos implementamos el método abstractivo, con ayuda de herramientas de inteligencia computacional. En este caso se utilizó un modelo Secuencia a Secuencia, con una capa de atención. El modelo Seq2Seq es el encargado de tomar un flujo de palabras y generar otro flujo, este proceso es mejorado utilizando una capa de atención, la cual va a tener en cuenta el contexto del flujo de entrada. Podemos observar que la síntesis generada con la reseña de ejemplo está acorde con el contexto del texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
